---
title: "Likelihoods"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r, echo = FALSE, message = FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
set.seed(100)

theme_set(theme_bw())
knitr::opts_chunk$set(fig.height = 3)
```

History of the course:

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

## Outline


# Likelihood Construction and Estimation

<br/><br/><br/><br/><br/>

Why do Statisticians love likelihood-based estimation?

1. <br/><br/><br/><br/>

2. <br/><br/><br/><br/>

3. <br/><br/><br/><br/>

4. <br/><br/><br/><br/>

Downsides?

1. <br/><br/><br/><br/>


1. <br/><br/>

## Introduction

**Definition: ** Suppose random variables $\boldsymbol Y = (Y_1, \dots Y_n)^\top$ has joint density or probability mass function $f_\boldsymbol Y(\boldsymbol y, \boldsymbol \theta)$ where $\boldsymbol \theta = (\theta_1, \dots, \theta_b)$. Then the *likelihood function* is
$$
L(\boldsymbol \theta | \boldsymbol Y) = f_\boldsymbol Y(\boldsymbol Y, \boldsymbol \theta).
$$

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Key concept: ** In all situations, the likelihood is the joint density of the observed data to be analyzed.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Notation

Given $\boldsymbol y$, note that $L(\boldsymbol \theta | \boldsymbol y): \mathbb{R}^b \rightarrow \mathbb{R}$.

<br/><br/><br/><br/><br/><br/><br/><br/>

Generally, we optimize $\ell(\boldsymbol \theta) = \log L(\boldsymbol \theta | \boldsymbol y)$.

<br/><br/><br/><br/>

How?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example:** Suppose we have $Y_1, \dots Y_n \stackrel{iid}{\sim} \text{Exp}(\lambda)$. The likelihood function is defined as

<br/><br/><br/><br/>

```{r}
# likelihood simulation
n <- 10
lambda <- 1

# plot of exponential(lambda) density
data.frame(x = seq(0, 8, .01)) |>
  mutate(f = dexp(x, rate = lambda)) |>
  ggplot() +
  geom_line(aes(x, f))
```

<br/><br/><br/><br/><br/><br/><br/><br/>

```{r}
# define likelihood
loglik <- function(lambda, data)
{
	lik <- prod(dexp(data, rate = lambda))
	loglik <- sum(dexp(data, rate = lambda, log = T))
	
	out <- data.frame(lik = lik, loglik = loglik)
	return(out)
}	

# simulate data
data <- rexp(n = n, rate = lambda)

# plot likelihood and loglikelihood
data.frame(lambda = seq(0, 3, by = .01)) |>
  rowwise() |>
  mutate(loglik = loglik(lambda, data)) |>
  unnest(cols = c(loglik)) |>
  pivot_longer(-lambda, names_to = "func", values_to = "vals") |>
  ggplot() +
  geom_vline(aes(xintercept = 1 / mean(data)), lty = 2) + # max likelihood estimate is 1/mean
  geom_line(aes(lambda, vals)) +
  facet_wrap(~func, scales = "free")

```
<br/><br/><br/><br/><br/><br/><br/><br/>

The likelihood function is random!

```{r, out.width="33%", fig.show='hold', fig.height=4, fig.width=4}
for(i in seq_len(3)) {
  # simulate data
  data <- rexp(n = n, rate = lambda)
  
  # plot likelihood and loglikelihood
  data.frame(lambda = seq(0, 3, by = .01)) |>
    rowwise() |>
    mutate(loglik = loglik(lambda, data)) |>
    unnest(cols = c(loglik)) |>
    ggplot() +
    geom_vline(aes(xintercept = 1 / mean(data)), lty = 2) + # max likelihood estimate is 1/mean
    geom_line(aes(lambda, loglik)) +
    theme(text = element_text(size = 20)) -> p ## make legible in notes
  
    print(p)
}

```

<br/><br/>

**Your Turn:** What is the effect of sample size on the log-likelihood function? Make a plot showing the log-likelihood function that results from $n = 10$ vs. $n = 100$ with corresponding MLE.

<br/><br/><br/><br/><br/><br/>

## Construction

The use of the likelihood function in parameter estimation is easiest to understand in the case of discrete iid random variables.

### Discrete IID Random Variables

Suppose each of the $n$ random variables in the sample $Y_1, \dots, Y_n$ have probability mass function $f(y; \boldsymbol \theta) = P_\boldsymbol \theta(Y_1 = y), y = y_1, y_2, \dots$. The likelihood is then defined as:
$$
L(\boldsymbol \theta | \boldsymbol Y) = \text{ joint density of observed random variables}
$$
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

In other words, 

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example (Fetal Lamb Movements): ** Data on counts of movements in five-second intervals of one fetal lamb ($n = 240$ intervals:)

```{r, echo=FALSE}
data.frame(no_movement = seq(0, 7),
           count = c(182, 41, 12, 2, 2, 0, 0, 1)) |>
  t() -> lambs
rownames(lambs) <- c("No. of Movements", "Count")

lambs |>
  kable(booktabs = TRUE)
```

Assume a Poisson model: $P(Y = y) = f_Y(y; \lambda) = \frac{\exp(-\lambda)\lambda^y}{y!}$. Then the likelihood is

<br/><br/><br/><br/><br/><br/>

Equating the derivative of the loglikelihood with respect to $\lambda$ to zero and solving results in the MLE

$$
\hat{\lambda}_\text{MLE} = \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
$$
<br/><br/>

This is the best we can do with this model. But is it good?

```{r, echo = FALSE}
data.frame(no_movement = seq(0, 7),
           count = c(182, 41, 12, 2, 2, 0, 0, 1)) -> lambs_model_df

lambda_hat <- lambs_model_df |>
  mutate(exp_count = no_movement * count) |>
  summarise(lambda_hat = sum(exp_count) / sum(count)) |>
  pull(lambda_hat)

lambs_model_df |>
  rowwise() |>
  mutate(Model = dpois(no_movement, lambda_hat) * sum(lambs_model_df$count)) |>
  rename(Observed = count) |>
  pivot_longer(-no_movement, names_to = "type", values_to = "count") |>
  ggplot() +
  geom_bar(aes(no_movement, count, fill = type), position = "dodge", stat = "identity") +
  xlab("# of movements") +
  scale_fill_discrete("")
              

```
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Multinomial Likelihoods

The multinomial distribution is a generalization of the binomial distribution where instead of 2 outcomes (success or failure), there are now $k \ge 2$ outcomes.

<br/><br/><br/><br/>

The probability mass function is

<br/><br/><br/><br/>

For $N_1, \dots, N_k, N_i =$ the number of balls in $i^\text{th}$ urn,

<br/><br/><br/><br/><br/><br/><br/><br/><br/>

The maximum likelihood estimator of $p_i$:

<br/><br/><br/><br/><br/><br/><br/>

More interesting multinomial likelihoods arise when the $p_i$ are modeled as a function of a lesser number of parameters $\theta_1, \dots, \theta_m$, $m < k - 1$.

**Example (Capture-Recapture):** To estimate fish survival during a specific length of time (e.g., one month), a common approach is to use a removal design.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>


### Continuous IID Random Variables

Recall: the likelihood is the joint density of data to be analyzed.

**Example (Hurricane Data):** For $36$ hurricanes that had moved far inland on the East Coast of the US in 1900-1969, maximum 24-hour precipitation levels during the time they were over mountains.

```{r echo=FALSE}
hurr_rain <- c(31, 2.82, 3.98, 4.02, 9.5, 4.5, 11.4, 10.71, 6.31, 4.95, 5.64, 5.51, 13.40, 9.72, 6.47, 10.16, 4.21, 11.6, 4.75, 6.85, 6.25, 3.42, 11.8, 0.8, 3.69, 3.10, 22.22, 7.43, 5, 4.58, 4.46, 8, 3.73, 3.5, 6.2, 0.67)

ggplot() + 
  geom_histogram(aes(hurr_rain), binwidth = .5) +
  xlab("Precipitation Levels")
```

We model the precipitation levels with a gamma distribution, which has density
$$
f(y; \alpha, \beta) = \frac{1}{\Gamma(\alpha) \beta^\alpha} y^{\alpha - 1}\exp(-y/\beta), \quad y > 0, \alpha, \beta > 0.
$$
This leads to the likelihood

<br/><br/><br/><br/><br/>

Of course, this cannot be interpreted as a probability because

<br/><br/><br/><br/>

To get a probability, need to go from a density to a measure.

<br/><br/><br/><br/>

But it may be useful to think of the value of the likelihood as being proportional to a probability.

<br/><br/><br/><br/><br/><br/><br/><br/>

More formally, begin with the definition of a derivative

$$
g'(x) = \lim\limits_{h \rightarrow0^+} \frac{g(x + h) - g(x - h)}{2h}.
$$

Let $F$ be the cumulative distribution function of a continuous random variable $Y$, then (if the derivative exists)

$$
f(y) =  \lim\limits_{h \rightarrow0^+} \frac{F(x + h) - F(x - h)}{2h} = \qquad \qquad \qquad \qquad \qquad
$$

If we substitute this definition of a density into the definition of the likelihood

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Compare this to the iid discrete case:


<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Example (Hurricane Data, Cont'd):** Recall with a gamma model, the likelihood for this example is
$$
L(\boldsymbol \theta | \boldsymbol Y) = \{\Gamma(\alpha)\}^{-n} \beta^{-n\alpha} \left\{\prod Y_i\right\}^{\alpha - 1} \exp\left(-\sum y_i/\beta\right),
$$
and log-likelihood
$$
\ell(\boldsymbol \theta) = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
$$

```{r, fig.height=2}
## loglikelihood function
neg_gamma_loglik <- function(theta, data) {
  -sum(log(dgamma(data, theta[1], scale = theta[2])))
}

## maximize
mle <- nlm(neg_gamma_loglik, c(1.59, 4.458), data = hurr_rain)
mle$estimate

## Gamma QQ plot
data.frame(theoretical = qgamma(ppoints(hurr_rain), mle$estimate[1], scale = mle$estimate[2]),
           actual = sort(hurr_rain)) |>
  ggplot() +
  geom_abline(aes(intercept = 0, slope = 1), colour = "grey") +
  geom_point(aes(theoretical, actual)) +
  xlab("Gamma percentiles") + ylab("Ordered values")
  
```

### Mixtures of Discrete and Continuous RVs

Some data $Y$ often have a number of zeros and the amounts greater than zero are best modeled by a continuous distribution. 

<br/>

Ex:

<br/>

In other words, they have positive probability of taking a value of exactly zero, but continuous distribution otherwise.

<br/><br/><br/>

A sensible model would assume $Y_i$ are iid with cdf
$$
F_Y(y; p, \boldsymbol \theta) = \begin{cases}
0 & y = 0 \\
p & y = 0 \\
p + (1 - p)F_T(y; \boldsymbol \theta) & y > 0
\end{cases}
$$
where $0 < p \le 1$ is $P(Y = 0)$ and $F_T(y; \boldsymbol \theta)$ is a distribution function for a continuous positive random variable.

Another way to write this:

<br/><br/><br/><br/><br/>

How to go from here to get a likelihood?


<br/><br/><br/><br/><br/>

One approach: let $n_0$ be the number of zeroes in the data and $m = n - n_0$ be the numbe rof non-zero $Y_i$. This leads to an intuitive way to contruct the likelihood for iid $Y_1, \dots, Y_n$ distributed according to the above distribution:
$$
L(\boldsymbol \theta | \boldsymbol Y) = \lim\limits_{h \rightarrow 0^+} \left(\frac{1}{2h}\right)^m \prod\limits_{i = 1}^n\{F_Y(Y_i + h; p, \boldsymbol \theta) - F_Y(Y_i - h; p, \boldsymbol \theta)\}
$$
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Feels a little arbitrary in how we are defining different weights on our likelihood for discrete and continuous parts.

<br/><br/>

Turns out, it doesn't matter! (Need some STAT 630/720 to see why.)

**Definition (Absolute Continuity)** On $(\mathbb{X}, \mathcal{M})$, a finitely additive set function $\phi$ is absolutely continuous with respect to a measure $\mu$ is $\phi(A) = 0$ for each $A \in \mathcal{M}$ with $\mu(A) = 0$. We also say $\phi$ is dominated by $\mu$ and write $\phi \ll \mu$. If $\nu$ and $\mu$ are measures such that $\nu \ll \mu$ and $\mu \ll nu$ then $\mu$ and $\nu$ are equivalent.

<br/><br/><br/><br/>

**Theorem (Lebesgue-Randon-Nikodym)** Assume that $\phi$ is a $\sigma$-finite countably additive set function and $\mu$ is a $\sigma$-finite measure. There exist unique $\sigma$-finite countably additive set functions $\phi_s$ and $\phi_{ac}$ such that $\phi = \phi_{ac} + \phi_s \ll \mu$, $\phi_s$ and $\mu$ are mutually singular and there exists a measurable extended real valued function $f$ such that
$$
\phi_{ac}(A) = \int_A f d\mu, \qquad \text{ for all } A \in \mathcal{M}.
$$
If $g$ is another such function, then $f = g$ a.e. wrt $\mu$. If $\phi \ll \mu$ then $\phi(A) = \int_Af d\mu$ for all $A \in \mathcal{M}$.

<br/>

**Definition (Radon-Nikodym Derivative)** $\phi = \phi_{ac} + \phi_s$ is called the Lebesgue decomposition. If $\phi \ll \mu$, then the density function $f$ is called the Radon-Nikodym derivative of $\phi$ wrt $\mu$.

<br/>

So what?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>



<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

### Proportional Likelihoods



### Empirical Distribution Function as MLE {.page-break-before}

### Censored Data

## Likelihood for Regression Models

## Marginal and Conditional Likelihoods

## The Maximum Likelihood Estimator and the Information Matrix

## Methods for Maximizing the Likelihood


